---
title: "Award-Winning Cotton Yield Prediction Workflow"
author: "Ashbin Bhat"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: united
---

```{r}
# =============================================================================
#  Environment Setup
# =============================================================================
# 1. Clear workspace
rm(list = ls())
```

```{r}
# 2. Set seed for reproducibility
set.seed(1996)
```

#Loading libraries

```{r}
#Loading necessary libraries

# --- Data Import and Basic Handling ---
library(readr)        # For reading CSV and delimited text files
library(janitor)      # For cleaning column names, removing empty rows/columns
library(naniar)       # Handling missing data

# --- Data Wrangling & Cleaning ---
library(dplyr)        # Data manipulation (filter, mutate, group_by, etc.)
library(tidyr)        # Data tidying (pivoting, reshaping)
library(lubridate)    # Date/time manipulation
library(tidyverse)    # Collection of packages for data science


# --- Visualization ---
library(ggplot2)      # Core plotting
library(ggthemes)     # Extra themes for ggplot2
library(ggpubr)       # Publication-ready plots
library(cowplot)      # Combining multiple ggplots
library(corrplot)     # Correlation matrix visualization
library(GGally)       # Pairwise plots (ggpairs)

# --- Machine Learning Framework ---
library(tidymodels)   # for parsnip, recipes, tune, etc.

# --- Modeling Engines ---
library(xgboost)      # XGBoost implementation
library(ranger)       # Fast Random Forests (optional alternative for comparison)

# --- Feature Engineering ---
library(recipes)      # Included in tidymodels, but useful to load for step-by-step prep
library(textrecipes)  # If you're working with text data

# --- Model Tuning & Validation ---
library(finetune)     # Advanced hyperparameter tuning strategies (racing, ANOVA, etc.)
library(tune)         # For grid/random tuning workflows
library(rsample)      # Data splitting (train/test, cross-validation)

# --- Metrics and Evaluation ---
library(yardstick)    # Evaluation metrics (RMSE, RÂ², MAE, etc.)
library(vip)          # Variable importance plots
library(pROC)         # ROC and AUC curves for classification (if needed)

# --- Parallel Processing ---
library(doParallel)   # Enable parallel computation to speed up tuning/resampling
library(future)

# --- Utilities & Diagnostics ---
library(car)          # Variance inflation factors, etc.
library (skimr)         # For a quick skim/summary of the data
library(DataExplorer) # For EDA
```

# 1.Data Import and Merging
```{r}
# -----------------------------------------------------------------------------
# 1.1 Read and aggregate daily weather to per-year/site
# -----------------------------------------------------------------------------
weather_daily <- read_csv("data/training/weather_data.csv")

weather_yearly <- weather_daily %>%
  group_by(year, site) %>%
  summarise(
    across(dayl_s:vp_pa, ~ mean(.x, na.rm = TRUE)),
    .groups = "drop"
  )

# -----------------------------------------------------------------------------
# 1.2 Read trait, soil, and meta datasets
# -----------------------------------------------------------------------------
trait <- read_csv("data/training/training_trait.csv")

soil <- read_csv("data/training/training_soil.csv") %>%
  mutate(site = sub("_\\d{4}$", "", site))  # remove year suffix

meta <- read_csv("data/training/training_meta.csv") %>%
  select(year, site, previous_crop, longitude, latitude)

# -----------------------------------------------------------------------------
# 1.3 Merge all into one cleaned dataframe
# -----------------------------------------------------------------------------
full_df <- trait %>%
  left_join(weather_yearly, by = c("year", "site")) %>%
  left_join(soil,          by = c("year", "site")) %>%
  left_join(meta,          by = c("year", "site")) %>%
  # Parse dates and engineer Days After Planting (DAP)
  mutate(
    date_planted   = mdy(date_planted),
    date_harvested = mdy(date_harvested),
    DAP            = as.numeric(date_harvested - date_planted)
  ) %>%
  drop_na()  # drop any remaining missing values

# Preview
glimpse(full_df)

```
#2. Exploratory Data Analysis
```{r}
# -----------------------------------------------------------------------------
# 2.1 Summary statistics
# -----------------------------------------------------------------------------
skimr::skim(full_df)

# -----------------------------------------------------------------------------
# 2.2 Boxplots for key numeric predictors
# -----------------------------------------------------------------------------
numeric_vars <- full_df %>% select(
  precp = prcp_mm_day, srad = srad_w_m_2,
  tmax = tmax_deg_c, tmin = tmin_deg_c,
  soilpH, om_pct, pk, DAP
)

numeric_vars_long <- numeric_vars %>%
  pivot_longer(everything(), names_to = "variable", values_to = "value")

ggplot(numeric_vars_long, aes(x = variable, y = value)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Distribution of Numeric Predictors",
       x = NULL, y = "Value")

```
#3. Feature Engineering & Recipe
```{r}
# -----------------------------------------------------------------------------
# 3.1 Train/Test Split
# -----------------------------------------------------------------------------
data_split <- initial_split(full_df, prop = 0.7, strata = yield_mg_ha)
train_df    <- training(data_split)
test_df     <-  testing(data_split)

# -----------------------------------------------------------------------------
# 3.2 Build preprocessing recipe
# -----------------------------------------------------------------------------
cotton_recipe <- recipe(yield_mg_ha ~ ., data = train_df) %>%
  # Remove identifiers and raw dates
  step_rm(year, site, replicate, block, date_planted, date_harvested) %>%
  # Impute any remaining numeric missing values with median
  step_impute_median(all_numeric_predictors()) %>%
  # Create Growing Degree Days (GDD) feature
  step_mutate(GDD = pmax((tmax_deg_c + tmin_deg_c) / 2 - 10, 0)) %>%
  # Encode categorical variables
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  # Remove highly correlated numeric predictors
  step_corr(all_numeric_predictors(), threshold = 0.9) %>%
  # Normalize all numeric predictors
  step_normalize(all_numeric_predictors())

# Preview recipe
cotton_recipe %>% prep() %>% juice() %>% glimpse()

```

#4. Model Specification & Tuning
```{r}
# -----------------------------------------------------------------------------
# 4.1 Specify XGBoost regression model with tunable parameters
# -----------------------------------------------------------------------------
xgb_spec <- boost_tree(
  trees          = tune(),
  tree_depth     = tune(),
  min_n          = tune(),
  learn_rate     = tune(),
  sample_size    = tune(),  # subsample ratio
  mtry           = tune(),  # predictors sampled per tree
  loss_reduction = tune()   # gamma
) %>%
  set_engine("xgboost", objective = "reg:squarederror") %>%
  set_mode("regression")

# -----------------------------------------------------------------------------
# 4.2 Assemble workflow
# -----------------------------------------------------------------------------
cotton_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(cotton_recipe)

# -----------------------------------------------------------------------------
# 4.3 Resampling strategy: 5-fold CV repeated 3 times
# -----------------------------------------------------------------------------
cv_splits <- vfold_cv(train_df, v = 5, repeats = 3)

# -----------------------------------------------------------------------------
# 4.4 Bayesian tuning control
# -----------------------------------------------------------------------------
bayes_ctrl <- control_bayes(
  verbose    = TRUE,
  no_improve = 10,
  save_pred  = TRUE
)

# -----------------------------------------------------------------------------
# 4.5 Parallel backend
# -----------------------------------------------------------------------------
registerDoParallel(cores = parallel::detectCores() - 1)

# -----------------------------------------------------------------------------
# 4.6 Run Bayesian hyperparameter tuning
# -----------------------------------------------------------------------------
xgb_tuned <- tune_bayes(
  cotton_wf,
  resamples = cv_splits,
  iter      = 30,
  metrics   = metric_set(rmse, rsq),
  control   = bayes_ctrl
)

# Stop parallel processing
foreach::registerDoSEQ()

```


#5. Finalize & Evaluate
```{r}
# -----------------------------------------------------------------------------
# 5.1 Select best parameters by RMSE
# -----------------------------------------------------------------------------
best_params <- select_best(xgb_tuned, "rmse")

# 5.2 Finalize workflow
final_wf <- finalize_workflow(cotton_wf, best_params)

# 5.3 Fit on training + evaluate on test fold
final_fit <- last_fit(final_wf, split = data_split)

# Collect metrics on hold-out set
final_metrics <- final_fit %>% collect_metrics()
print(final_metrics)

# 5.4 Predictions vs Observed
preds <- final_fit %>% collect_predictions()

ggplot(preds, aes(x = yield_mg_ha, y = .pred)) +
  geom_point(alpha = 0.6) +
  geom_abline(linetype = "dashed") +
  labs(
    title = "Observed vs. Predicted Cotton Yield",
    x = "Observed Yield (Mg/ha)",
    y = "Predicted Yield (Mg/ha)"
  )

```

#6. Variable Importance
```{r}
# Fit final model on full training data
final_model_fit <- fit(final_wf, data = train_df)

# Extract and plot variable importance
vip::vip(final_model_fit$fit$fit, num_features = 15) +
  labs(title = "Top 15 Variable Importances")

```





